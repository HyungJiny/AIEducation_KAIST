# DeepLearning
## 곽병권 대표
---
deep하다 - layer 수를 늘린다.
wide하다 - hidden layer의 output을 늘린다.

Tensorboard를 통해 각 layer를 시각화하여 확인할 수 있다.

layer를 무작정 늘렸더니 학습이 이루어지지 않는 문제 발생
일정 cost 이하로 떨어지지 않음
이를 해결하기 위해 Backpropagation을 이용하기 시작 
Vanishing gradinent 문제 발생 - 경사가 애매해지기 시작하여 sigmoid가 의미가 없어짐

### ReLu
sigmoid를 변형한 모델
중간 layer에 ReLu를 적용하고, 결과를 위해 마지막엔 sigmoid를 적용하면 학습이 됨

딥러닝의 가장 큰 문제는 실험적으로는 결과가 나오지만, 결과에 대한 검증이 없다. 
동작에 대한 이해 및 검증은 시도하고 있는 현실 

### 초기값 문제 
초기값에 대한 정답이 나와있지는 않음 
Restricted Boatman Machine(RBM) - 초기값을 pretraining 시키는 모델 
X를 가지고 forward 연산 후 값을 구하고, backward 연산을 다시 함
이를 통해 X가 들어갔다가 다시 나오는 W와 b를 먼저 구하는 방식 
하지만, 복잡하고 비용이 커서 잘 사용하지 않게됨

입력의 갯수와 출력의 갯수 사이 값을 구하고 입력의 제곱으로 나누었더니 잘 되더라...
그 후, 입력의 1/2의 제곱을 구하고 해봤더니 더 잘되더라...
왜인지는 모름

### Dropout
학습시 주어진 데이터를 완전히 외우는 경우가 생김
오버피팅을 증명하는 방법은 Training Error와 Testing Error를 비교했을 때, 가장 차이가 나지 않는 구간을 넘어가면 오버피팅
Training Error 학습 데이터를 가지고 에러 측정
Testing Error 테스트 데이터를 가지고 에러 측정

이전에 정규화를 통해 각각의 W가 높은 값을 가지지 않도록 패널티를 적용 
path 한 번 지날 때 마다 랜덤하게 일부 데이터를 빼고 학습하는 방법이 Dropout
네트워크의 융통성을 통해 작동하게 됨
코딩할 땐 dropout_rate를 지정하면 됨 
학습시킬 때는 dropout_rate를 변수로 적용, 실험을 할 때는 dropout_rate를 1로 설정(예측을 진행할 때는 전력을 다해야하기 때문에)

