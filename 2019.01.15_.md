#
## 곽병권 대표
---
deep하다 - layer 수를 늘린다.
wide하다 - hidden layer의 output을 늘린다.

Tensorboard를 통해 각 layer를 시각화하여 확인할 수 있다.

layer를 무작정 늘렸더니 학습이 이루어지지 않는 문제 발생
일정 cost 이하로 떨어지지 않음
이를 해결하기 위해 Backpropagation을 이용하기 시작 

### ReLu
중간 layer에 ReLu를 적용하고, 결과를 위해 마지막엔 sigmoid를 적용하면 학습이 됨
