# CNN
## 곽병권 대표 
---
교양이 뇌 실험에서부터 신경망 아이디어 시작 

`conv > relu > conv > relu > pool` 을 반복하는 형태

### Convolution Layer
작은 필터를 합성함, 작은 필터를 대입해서 하나의 수를 뽑아냄 
x의 채널과 필터의 채널은 동일해야함 
stride가 1인 경우, `(n-f)/f + 1` 의 수로 나옴

패딩을 하는 경우 필터를 적용해도 사이즈에는 변환이 없음 
그림의 끝부분을 알려줄 수 있음 

레이어에서의 파라미터 갯수는 메모리에 영향을 주기 때문에 필요함 

가장 흔하게 사용되는 필터는 3x5x5
처음 생성되는 w는 랜덤값(초기화)
진행하면서 backpropagation을 통해 수정해나감 

split에 merge를 한 형태가 cnn의 형태

### Pooling layer
output 사이즈가 줄어들지 않는 것은 좋지만, 너무 오래 가져가면 좋지 않을 수 있다.
output을 줄이는 것이 목적 
downset pooling을 하는 역할 

가장 많이 사용하는 방법이 Max Pooling
대표값을 가지고 가겠다는 것이 목표
크기를 절반정도 줄여줌 

W가 필요 없음
파라미터가 전혀 사용되지 않는다.
F=2, S=2 / F=3, S=2 를 이용하면 보통 절반정도로 떨어짐

신경망의 특성상 병렬 처리가 가능하기 떄문에 GPU 연산이 가능해진다.

### AlexNet
처음으로 ReLu를 사용함
batch size가 128로 작음
SGD를 사용 - 데이터를 다 안쓰기 때문에 cost가 산만하게 나옴
100만장에 GD를 모두 사용할 수 없다면 랜덤으로 sampling을 해서 일부만 사용하겠다. 
빠른 학습을 위함 
SGD를 적용하면 optimizer가 중요해짐 
그 당시 학습하는데 2주 걸림
따라서 중간중간에 save를 하면서 학습을 진행하는 것이 중요 

**tesorflow 코드를 활용할 때, 개념만 잘 알면 파라미터로 모든 설정 값을 넘겨줄 수 있기 때문에 어렵지 않게 작성할 수 있다.**
keras를 이용하면 더 짧게 구현 가능 
각 layer를 어떻게 구성하고, 어떤 동작을 해야하고, input이 무엇인지 잘 아는게 중요함.
즉, 모델에 대한 이해가 가장 중요 
