# 딥러닝 프레임워크
## 곽병권 대표
---
**실습 자료 : <http://github.com/Steven-A3/DeepLearningZeroToAllColab>**

소프트 2.0 : 학습을 통해 스스로 결과를 추론해 내는 것
ex) 웹 페이지 양식을 주면 html, javascript를 스스로 코딩해주는 프로그램 

지도 학습 - Label이 존재하는 데이터 
비지도 학습 - Label이 존재하지 않는 데이터 

머신러닝에는 데이터가 가장 중요하다.
Tool이 아무리 있어도 정답이 필요하기 때문에 Labeling을 진행하는데 사람을 많이 사용함
아직 지도학습이 가장 좋은 결과를 내놓고, 비지도 학습은 잘 사용되지 않고 있기 때문에
현실에 존재하는 인공지능의 한계점. 그래도 지도 학습마저 못하고 있는 이전보다는 낫다.

### TensorFlow
성능을 위해 기본 구현은 C로 되어 있음
가장 많은 사람들이 활용하고 있는 라이브러리 
현재 pytorch도 많이 따라오고 있음 
하지만 tensorflow만 알고 있어도 대부분 머신러닝을 해결할 수 있다.
학습하는 방법만 알면 된다.
**기본 컨셉을 이해하는 것이 가장 중요**
러닝을 하는 최적의 방법은 난수를 지정하고, 이를 조금씩 변경해 가는 것이 가장 좋다. 

- Data Flow Graph
    - 수학 연산을 그래프로 표현해 놓은 과정 
    - 그래프를 통해 연산을 진행하는 것 

- Tensor : 학습을 위한 데이터 형태(Vector)
실행을 위해서는 session이 필요함 

- placeholder : 변수를 위한 자리만 생성하는 것
스칼라 값, n차원 배열 등 어느 형태로든 데이터가 들어갈 수 있음
유연하게 값이 들어갈 수 있도록 해줌 

- Rank : 차원 

기본 구조를 만들어 놓고 learning을 진행할 때, session을 run 하면서 feed_dict 파라미터를 통해 X, Y 등 데이터 값을 넣어준다.

session run에 학습 파라미터를 배열로 넣으면 넣은 순서대로 값이 반환됨

*기본 순서*
1. 모델(가설)을 빌드, cost를 선언 
2. session을 만들고 데이터를 넣어 실행 
3. 학습 진행 과정에서 값이 변함

---

### 비용 줄이기 
cost의 그래프를 그리면 밥그릇 모양의 그래프가 나옴(Convex 함수)

**Gradient Descent Algorithm(경사하강법)**
cost가 줄어드는 방향으로 W를 움직이는 방법 
local minimun을 찾을 때까지 반복한다.
global minimun을 찾는 것은 사실상 불가능
cost가 여러개인 경우에도 사용 가능 

경사 미분 값을 cost에 곱하는 방법의 공식 

데이터가 복잡해질 수록 특정 local minimun을 뛰어넘기 위해 복잡한 알고리즘을 적용함 
minimun 값을 찾기 위한 목적은 같음 

### 변수가 여러개인 경우 
변수의 수 만큼 w가 늘어나면 됨
cost는 동일하게 구할 수 있음 
행렬을 이용하면 같은 결과를 만들 수 있으므로 행렬을 이용함 
-> 인스턴스가 늘어나면 행을 늘려주면 됨 
복잡한 수식을 간단하게 만들어 줌 H(X) = WX

코드를 구현할 때, input과 W의 shape를 결정하면서 사용하게 되므로 모양을 잘 이해해야함 
W의 행의 수를 결정하는 것은 feature의 갯수 

현실 문제에 적용하면 output이 1개만 나오지 않고, 여러가지로 나올 수 있다.
feature의 갯수와 출력의 갯수를 통해 W 행렬이 결정됨 
-> `W = [feature의 수, output의 수]` 모양의 행렬

### 학습 
learning_rate는 경험적으로 주어진 모델, 데이터셋에 대해 해봤던 것 중 가장 좋은 값으로 고름 
수학적으로 증명할 수 있는 방법은 아님
단지 해보니 가장 좋은 것을 이용 

---

### 분류 문제
논리 함수 `시그모이드 함수(sigmoid function)`
H(x) = WX + b 를 0~1 값으로 나타내고 싶은 경우 
H(x) = z 로 만들고 이를 g(z)로 했을 때, 0~1 로 결과값이 나옴
이 때, 함수 g가 sigmoid 함수
**즉, 결과 값 Y를 0~1로 표현해주는 함수**

하지만, cost 함수의 모양이 울퉁불퉁한 모양으로 변함 
이로 인해 local minimun이 많아지고, 올바른 학습이 되지 않음 
따라서 새로운 비용 함수를 설계함 => log를 이용하여 울퉁불퉁한 부분을 펴자
결과의 참, 거짓에 따라 수식을 변화시켜 틀린 경우 cost가 무한정 높아지게 만듦 
이 결과 똑같은 경사 하강법을 적용시킬 수 있게 됨 

### 다항 분류 문제 
이진 분류기를 클래스 수 만큼 돌리면 됨 
복잡하게 생각할 필요 없음 

X는 같으나 W가 달라지는 문제
feature 수 만큼의 열, class 만큼의 행을 가진 W를 이용하여 WX를 구하면 학습이 가능
나온 결과값에 하나하나 sigmoid를 씌우고 결과를 찾기보다 한번에 나왔으면 좋겠음
WX를 통해 나온 결과를 `softmax`에 집어 넣자. 그리고 `argmax`를 통해 최대값을 찾고 `onehot-encoding`을 이용하여 제일 큰 값으로 분류하자

cost를 구하기 위해 cross entropy를 사용 

참고, tensorflow를 이용하여 onehot 인코딩을 할 경우 level을 하나 늘려주기 때문에 이를 제거하기 위한 reshape을 해주어야한다.

---

### 학습률 
비용 함수의 결과를 관찰하고 시행착오를 해보는 방법밖에 없음 

**오버피팅(Overfitting)**
학습 데이터에 대해서는 높은 정확도를 가지지만 테스트 데이터에는 낮은 정확도를 보임 
데이터 양이 많을 수록 오버피팅이 일어나지 않음
`정규화`를 이용하여 오버피팅을 방지 해야함
>정규화 : 각각의 가중치 값(W)들이 너무 큰 수를 갖지 않도록 하는 것

기존 데이터를 학습 데이터와 테스트 데이터로 분류하여 학습을 시도
Validation(검증) 데이터를 이용하는 경우는 Training Set이 너무 큰 경우 학습을 하면서 Validation 데이터로 검증을 진행하면서 학습을 함 

batch를 사용하는 경우가 있는데 이는 한 번에 데이터를 메모리에 올릴 수 없을 때 사용하는 한 번에 읽을 크기 단위
epoch는 데이터 연산을 몇 번 진행할 것인가를 정하는 것.
데이터에 대한 전체 연산을 몇 번 진행할 것인가 
iteration은 하나의 epoch를 하기 위한 데이터 수
